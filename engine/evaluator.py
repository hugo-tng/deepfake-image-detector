import json
import torch
from tqdm import tqdm
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, classification_report, confusion_matrix
)

from utils.visualization import *

@torch.no_grad()
def evaluate_test_set(model, test_loader, device, output_dir):
    """Ch·∫°y ƒë√°nh gi√° tr√™n t·∫≠p test"""
    print("\n" + "="*50)
    print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å TR√äN T·∫¨P TEST")
    print("="*50)
    
    all_preds = []
    all_labels = []
    all_probs = [] # L∆∞u x√°c su·∫•t l·ªõp 1 (Fake) ƒë·ªÉ v·∫Ω ROC
    
    # 1. V√≤ng l·∫∑p d·ª± ƒëo√°n
    for images, labels in tqdm(test_loader, desc="Testing"):
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward
        outputs = model(images)
        
        # L·∫•y x√°c su·∫•t (Softmax)
        probs = torch.softmax(outputs, dim=1)
        
        # L·∫•y nh√£n d·ª± ƒëo√°n
        _, preds = torch.max(outputs, 1)
        
        # Gom k·∫øt qu·∫£
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        all_probs.extend(probs[:, 1].cpu().numpy()) # L·∫•y c·ªôt index 1 (Fake)
    
    # 2. T√≠nh to√°n Metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    
    print(f"\nüìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å:")
    print(f"   Accuracy : {accuracy:.4f}")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall   : {recall:.4f}")
    print(f"   F1 Score : {f1:.4f}")
    
    print("\nüìã Chi ti·∫øt theo l·ªõp:")
    print(classification_report(all_labels, all_preds, target_names=['Real', 'Fake']))

    # 3. Tr·ª±c quan h√≥a & L∆∞u
    # T·∫°o th∆∞ m·ª•c evaluation ri√™ng b√™n trong th∆∞ m·ª•c experiment
    eval_dir = os.path.join(output_dir, 'evaluation_results')
    os.makedirs(eval_dir, exist_ok=True)
    
    # A. Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)
    plot_confusion_matrix(cm, classes=['Real', 'Fake'], save_dir=eval_dir)
    
    # B. ROC Curve & AUC
    auc_score = plot_roc_curve(all_labels, all_probs, save_dir=eval_dir)
    print(f"=== ROC AUC Score: {auc_score:.4f}")
    
    # C. L∆∞u k·∫øt qu·∫£ d·∫°ng JSON
    results = {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'auc_score': float(auc_score),
        'confusion_matrix': cm.tolist()
    }
    
    json_path = os.path.join(eval_dir, 'test_metrics.json')
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=4)
        
    print(f"\nƒê√£ l∆∞u to√†n b·ªô k·∫øt qu·∫£ ƒë√°nh gi√° t·∫°i: {eval_dir}")